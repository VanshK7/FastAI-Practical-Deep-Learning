{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predicting Titanic Survivability with Random Forests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install fastai\n",
    "!conda install graphviz\n",
    "!conda install python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *            # Imports all the necessary libraries for ML like pandas, numpy, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests require significantly less data preprocessing (inc. cleaning) which is why Random Forests are easy to implement and hard to mess up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('titanic')\n",
    "df = pd.read_csv(path/'train.csv')\n",
    "test_df = pd.read_csv(path/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = df.mode().iloc[0]       # Calculates the mode of each column and stores it in modes\n",
    "modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to create dummy variables like we did in linear regression.\n",
    "\n",
    "Instead, we can convert these fields to categorical variables. Internally, pandas replaces each unique value with a specific number, which acts as the index for looking up the values in the list of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for data preprocessing\n",
    "def proc_data(df):\n",
    "    # Not necessary to calculate the LogFare, we do it to make the graph more 'distributed\n",
    "    df['Fare'] = df.Fare.fillna(0)          # Replacing NA values in Fare with 0\n",
    "    df.fillna(modes, inplace=True)          # Replacing other NA values with their modes\n",
    "    df['LogFare'] = np.log1p(df['Fare'])    # .log1p() calculates the log of Fare + 1\n",
    "    df['Embarked'] = pd.Categorical(df.Embarked)    # Converting Embarked into a categorical variable\n",
    "    df['Sex'] = pd.Categorical(df.Sex)\n",
    "    \n",
    "# Preprocessing training and testing data\n",
    "proc_data(df)\n",
    "proc_data(test_df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We treat `Pclass` as an ordered variable instead of a categorical variable here. As 1st class is better than 2nd, 2nd class is better than 3rd, and so on. So internally, the `Pclass` value won't be treated as independent values but ordered values.\n",
    "\n",
    "In decision trees, order matter a lot. They care only about order and not the mean absolute value/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the list of columns into Categorical, Dependent and Continuous.\n",
    "\n",
    "Continuous variables are variables which can accept any value in a given range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['Sex', 'Embarked']                                  # Categorical variables\n",
    "conts = ['Age', 'SibSp', 'Parch', 'LogFare', 'Pclass']     # Continuous variables\n",
    "dep = 'Survived'                                            # Dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While categorical values show their actual values (e.g. S, C, Q), internally they are stored as numbers which act as indexex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Embarked.head()             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the integers associated with different values in Embarked\n",
    "df.Embarked.cat.codes.head()            # 2 -> S, 0 -> C, 1 -> Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Binary Splits**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests are built on decision trees, and to create a decision tree, we need to create a binary split\n",
    "\n",
    "A binary split segregates the rows into 1 of 2 groups based on whether they're above or below a certain threshold.\n",
    "\n",
    "E.g. We can split the rows into Male and Female by using the threshold 0.5 for the `Sex` column (0 corresponds to Female and 1 to Male). Let's see how that would split up our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(11, 5))      # Creates a figure  1 row and 2 columns (2 plots) of width 11 and height 5 inches\n",
    "# fig stores the figure and axs holds\n",
    "\n",
    "sns.barplot(data=df, x='Sex', y=dep, ax=axs[0]).set(title=\"Survival Rate\")              # axs[0] refers to the 1st plot\n",
    "\n",
    "sns.countplot(data=df, x='Sex', ax=axs[1]).set(title=\"Histogram\")\n",
    "\n",
    "# Barplot displays the mean of a variable while countplot displays the total count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "- Survival rate is > 0.7 (70%) for females while < 0.2 (20%) for males\n",
    "- There are around 300 female passengers as compared to close to 600 male passengers\n",
    "\n",
    "Data suggests that females are significantly more likely to survive than males. We can create a very basic model that predicts that all females survive and no males do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and validation set\n",
    "from numpy import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "trn_df, val_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Replacing category variable values with their corresponding integer values (necessary for binary split)\n",
    "trn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)        \n",
    "val_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to define the dependent (xs - multiple dependent variables) and independent variables (y)\n",
    "def xs_y(df):\n",
    "    xs = df[cats+conts].copy()      # Dependent variables include all categorical and continuous variables\n",
    "    y = df[dep]\n",
    "    return xs, y\n",
    "\n",
    "trn_xs, trn_y = xs_y(trn_df)\n",
    "val_xs, val_y = xs_y(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the predictions for our basic model (where Female=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = val_xs.Sex == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using mean absolute error to see how good our model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "f\"{mean_absolute_error(val_y, preds):.3f}\"          # Rounding off to 3 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try splitting a regular column: `LogFare` and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fare = trn_df[trn_df.LogFare>0]              # Ensuring only valid fares are stores (in case there are some anomalies in the data)\n",
    "fig, axs = plt.subplots(1,2, figsize=(11,5))\n",
    "sns.boxenplot(data=df_fare, x=dep, y='LogFare', ax=axs[0])        # boxenplot is an enhanced version of boxplot\n",
    "sns.kdeplot(data=df_fare, x='LogFare', ax=axs[1])       # kdeplot is a kernel distribution plot\n",
    "\n",
    "# kdeplot is used to visualize the distribution of observations. It's like a histogram but it smoothens the data, making it easier to observe the shape and recognize any outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The horizontal line going across the box indicates the average `LogFare` for a particular condition.\n",
    "\n",
    "- The average `LogFare` for a passenger that died is around 2.5\n",
    "- The average `LogFare` for a passsenger that survived is around 3.25\n",
    "- It implies that people who bought more expensive tickets were more likely to survive\n",
    "\n",
    "We can create a model based on this observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = val_xs.LogFare > 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{mean_absolute_error(val_y, preds):.3f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the mean absolute error is higher, this model is less accurate than the model that used `Sex`.\n",
    "\n",
    "We can try out different splits and compare the performance by creating a `score` function.\n",
    "\n",
    "In the `score` function, we will not be returning the mean absolute error, we will return the *impurity*. When 2 groups are created after binary splitting, impurity tells us how similar or dissimilar rows in the same group are to each other.\n",
    "\n",
    "We can measure the similarity of a group by calculating it's standard deviation of the dependent variable. Then, we multiply the standard deviation by the number of rows (as more rows have a bigger impact than a smaller number of rows).\n",
    "\n",
    "A higher standard deviation indicates that the rows are different to each other, while a low standard deviation indicates that the rows are similar. Groups with lower standard deviation are likely to be more predictive as the data is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the score of 1 side of the binary split\n",
    "\n",
    "def _side_score(side, y):\n",
    "    tot = side.sum()            # Finding the total number of rows in one side of the binary split\n",
    "    if tot<=1:\n",
    "        return 0                # If there are 0 or just 1 rows in one side of the binary split, we return 0, as the data is too small to calculate the score\n",
    "    side_score = y[side].std()*tot      # Calculating the score, .std() calculates the standard deviation\n",
    "    return side_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the score of the entire binary split by adding up LHS and RHS side scores\n",
    "\n",
    "def score(col, y, split):\n",
    "    lhs = col<=split            # lhs = columns on the LHS of the split value\n",
    "    rhs = col>split             # rhs = columns on the RHS of the split value\n",
    "    score = (_side_score(lhs, y) + _side_score(rhs, y))/len(y)      # We divide by the total number of rows to normalize the score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the impurity of the 'Sex' column\n",
    "score(trn_xs['Sex'], trn_y, 0.5)        # Split is at 0.5 as 0 indicates Female and 1 indicates Male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the impurity of the 'LogFare' column\n",
    "score(trn_xs[\"LogFare\"], trn_y, 2.5)            # 2.5 as average LogFare of passenger who died is 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again come to our previous conclusion, splitting on `Sex` is a better idea than splitting on `LogFare` as the score of `LogFare` column is higher than the score of `Sex` column.\n",
    "\n",
    "Reminder: we aim for a lower score, not a higher one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically calculating the best split for a particular column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = trn_xs['Age']\n",
    "unq = col.unique()          # Finding all the unique values for a column\n",
    "unq.sort()\n",
    "unq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array([])\n",
    "for o in unq:\n",
    "    if not np.isnan(o):\n",
    "        scores = np.append(scores, score(col, trn_y, o))  # Calculating the score for each split which is not NaN\n",
    "\n",
    "# Finding the unique value for the lowest score\n",
    "unq[np.argmin(scores)]  # np.argmin() returns the index position of the minimum score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 5 is the ideal split for the `Age` column.\n",
    "\n",
    "We can write a function that automatically calculates the ideal split for a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_col(df, nm):\n",
    "    col, y = df[nm], df[dep]\n",
    "    unq = col.dropna().unique()         # Dropping all the NA values\n",
    "    \n",
    "    scores=np.array([])\n",
    "    for o in unq:\n",
    "        scores = np.append(scores, score(col, y, o))\n",
    "        \n",
    "    idx = scores.argmin()               # index value of the minimum score\n",
    "    return unq[idx], scores[idx]        # Returning the column value for ideal split and the corresponding score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate it for Age\n",
    "min_col(trn_df, \"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideal split is for `Age` column is 5 and the corresponding minimum score is 0.479.\n",
    "\n",
    "Now let's try it for all the columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = cats + conts     # Categorical + Continuous variables form all the independent variables\n",
    "\n",
    "result = {}             # A dictionary to store the results. Key = column name and value = column index for min. score and the corresponding min. score\n",
    "for o in cols:\n",
    "    result[o] = min_col(trn_df, o)\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "- Splitting `Sex` at 0 is our best binary split\n",
    "- Splitting `Parch` at 0 is the worst binary split (out of the best binary split for each column)\n",
    "- So `Parch` can be considered the least important column while `Sex` is the most important column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating a Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we improve our model?\n",
    "\n",
    "We can take each of the split: Male and Female, and create one more split for them. Find the single best split for Female and the single best split for Male and split the 2 groups accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.remove(\"Sex\")              # Removing Sex column from possible splits as we've already used it\n",
    "ismale = trn_df.Sex==1          # Passenger is male if Sex = 1\n",
    "males, females = trn_df[ismale], trn_df[~ismale]        # ~ is used for flipping binary values, so ~ismale indicates all rows are not male"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the best split for males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_split = {}\n",
    "for o in cols:\n",
    "    male_split[o] = min_col(males, o)\n",
    "    \n",
    "male_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting at `Age` column where Age = 6 is the single best split for the `Male` group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_split = {}\n",
    "for o in cols:\n",
    "    female_split[o] = min_col(females, o)\n",
    "    \n",
    "female_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting `Pclass` column where Pclass = 2 is the single best split for the `Female` group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a simple decision tree by adding these rules.\n",
    "\n",
    "The model will first split at `Sex` depending whether Sex is Male or Female. If Male, the tree will further split at `Age` depending on whether Age <= 6 or not. If Female, the tree will further split at `Pclass` depending on whether Pclass <= 2 or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of creating a decision tree can be automated using `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "# Creating a decision tree with max leaf nodes (max possible endpoints after the final split) as 4 and training it on our training data\n",
    "m =  DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "def draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n",
    "    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n",
    "                      special_characters=True, rotate=False, precision=precision, **kwargs)\n",
    "    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = os.pathsep + \"C:\\\\Users\\\\Vansh\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\graphviz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_tree(m, trn_xs, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
